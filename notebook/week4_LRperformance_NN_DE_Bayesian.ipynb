{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install blitz-bayesian-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################\n",
    "\n",
    "# 1. Logistic regression and basic NN with deep ensembles\n",
    "\n",
    "######################################\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Logistic regression\n",
    "# use multiple layers NN\n",
    "class LogisticDNN(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(LogisticDNN, self).__init__()\n",
    "        self.linear1 = nn.Linear(5, 60)\n",
    "        self.linear2 = nn.Linear(60, 20)\n",
    "        self.linear3 = nn.Linear(20, 10)\n",
    "        self.linear4 = nn.Linear(10, 5)\n",
    "        self.linear5 = nn.Linear(5, 1)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.linear3(x)\n",
    "        x = self.linear4(x)\n",
    "        x = self.linear5(x)\n",
    "        return x\n",
    "\n",
    "NNmodel = LogisticDNN()\n",
    "\n",
    "DEmodel = LogisticDNN()\n",
    "\n",
    "# define a loss function and optimizers\n",
    "criterion = nn.MSELoss()\n",
    "NN_optimizer = optim.SGD(NNmodel.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################\n",
    "\n",
    "# 2. Bayesian NNs on logistic regression\n",
    "\n",
    "######################################\n",
    "\n",
    "# A simple example for regression\n",
    "\n",
    "# Importing the necessary modules\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from blitz.modules import BayesianLinear\n",
    "from blitz.utils import variational_estimator\n",
    "\n",
    "\n",
    "# Creating our Logistic regression class with bayesian layers NN\n",
    "\n",
    "@variational_estimator\n",
    "class BayesianLogistic(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        # self.linear = nn.Linear(input_dim, output_dim)\n",
    "        self.blinear1 = BayesianLinear(input_dim, 100)\n",
    "        self.blinear2 = BayesianLinear(100, 20)\n",
    "        self.blinear3 = BayesianLinear(20, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.blinear1(x)\n",
    "        x2 = self.blinear2(x1)\n",
    "        return self.blinear3(x2)\n",
    "\n",
    "regressor = BayesianLogistic(5, 1)\n",
    "optimizer = optim.SGD(regressor.parameters(), lr=0.01)\n",
    "criterion = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1\n",
      "Accuracy of the 100 train numbers of linear separable dataset (LR): 98 %\n",
      "Accuracy of the 100 test numbers of linear separable dataset (LR): 100 %\n",
      "Finished training basic NN:\n",
      "Accuracy of the 100 train numbers of linear separable dataset (NN): 89 %\n",
      "Accuracy of the 100 test numbers of linear separable dataset (NN): 84 %\n",
      "Finished training Deep ensembles:\n",
      "Accuracy of the 100 train numbers of linear separable dataset (DE): 96 %\n",
      "Accuracy of the 100 test numbers of linear separable dataset (DE): 99 %\n"
     ]
    }
   ],
   "source": [
    "######################################\n",
    "\n",
    "# 1.1 Performance test on Linear separable dataset\n",
    "\n",
    "######################################\n",
    "\n",
    "print('1.1')\n",
    "\n",
    "# use sklearn to generate separable data\n",
    "from sklearn.datasets import make_classification\n",
    "separable = False\n",
    "while not separable:\n",
    "    samples = make_classification(n_samples=200, n_features=5, n_informative=2, n_redundant=0)\n",
    "    red = samples[0][samples[1] == 0]\n",
    "    blue = samples[0][samples[1] == 1]\n",
    "    separable = any([red[:, k].max() < blue[:, k].min() or red[:, k].min() > blue[:, k].max() for k in range(5)])\n",
    "\n",
    "# split data\n",
    "train_x = samples[0][0:100,:]\n",
    "train_y = samples[1][0:100]\n",
    "test_x = samples[0][100:200,:]\n",
    "test_y = samples[1][100:200]\n",
    "\n",
    "\n",
    "##############################\n",
    "# test on logistic regression\n",
    "##############################\n",
    "LR = LogisticRegression(solver='liblinear', random_state=0).fit(train_x, train_y)\n",
    "# accuracy score for train data\n",
    "print('Accuracy of the 100 train numbers of linear separable dataset (LR): %d %%' %\n",
    "      (100 * LR.score(train_x, train_y)))\n",
    "# predicted accuracy score for test data\n",
    "print('Accuracy of the 100 test numbers of linear separable dataset (LR): %d %%' %\n",
    "      (100 * LR.score(test_x, test_y)))\n",
    "\n",
    "##############################\n",
    "# test on basic neural network\n",
    "##############################\n",
    "\n",
    "# make dataset iterable\n",
    "train_loader_x = Variable(torch.Tensor(train_x))\n",
    "train_loader_y = Variable(torch.Tensor(train_y))\n",
    "test_loader_x = Variable(torch.Tensor(test_x))\n",
    "test_loader_y = Variable(torch.Tensor(test_y))\n",
    "\n",
    "# model training\n",
    "iter = 0\n",
    "for epoch in range(5):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(zip(train_loader_x,train_loader_y)):\n",
    "        inputs, labels = data\n",
    "        # reshape train data y\n",
    "        labels = labels.view(1)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        NN_optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        outputs = NNmodel(inputs)\n",
    "        # Compute Loss\n",
    "        NN_loss = criterion(outputs, labels)\n",
    "        # Backward pass\n",
    "        NN_loss.backward()\n",
    "        # optimize\n",
    "        NN_optimizer.step()\n",
    "\n",
    "        #print statistics\n",
    "        # running_loss += loss.item()\n",
    "        # if i % 20 == 19:  # print every 20 mini-batches\n",
    "        #      print('[%d, %5d] loss: %.3f' %\n",
    "        #            (epoch + 1, i + 1, running_loss / 2000))\n",
    "        #      running_loss = 0.0\n",
    "\n",
    "print('Finished training basic NN:')\n",
    "\n",
    "# make predictions\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in zip(train_loader_x,train_loader_y):\n",
    "        inputs, labels = data\n",
    "        # reshape test data y\n",
    "        labels = labels.view(1)\n",
    "\n",
    "        outputs = NNmodel(inputs)\n",
    "\n",
    "        predicted = torch.round(outputs)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "print('Accuracy of the 100 train numbers of linear separable dataset (NN): %d %%' % (\n",
    "    100 * correct / total))\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in zip(test_loader_x,test_loader_y):\n",
    "        inputs, labels = data\n",
    "        # reshape test data y\n",
    "        labels = labels.view(1)\n",
    "\n",
    "        outputs = NNmodel(inputs)\n",
    "\n",
    "        predicted = torch.round(outputs)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "print('Accuracy of the 100 test numbers of linear separable dataset (NN): %d %%' % (\n",
    "    100 * correct / total))\n",
    "\n",
    "##############################\n",
    "# Training an ensemble of 5 MLPs with MSE\n",
    "##############################\n",
    "de = []\n",
    "de_optimizers = []\n",
    "for _ in range(5):\n",
    "    de.append(DEmodel)\n",
    "    de_optimizers.append(torch.optim.SGD(params=DEmodel.parameters(), lr=0.001))\n",
    "# train\n",
    "for i, net in enumerate(de):\n",
    "    # print('Training network ',i+1)\n",
    "    for epoch in range(5):\n",
    "        for j, data in enumerate(zip(train_loader_x, train_loader_y)):\n",
    "            inputs, labels = data\n",
    "            # reshape train data y\n",
    "            labels = labels.view(1)\n",
    "\n",
    "            de_optimizers[i].zero_grad()\n",
    "            # reshape train data y\n",
    "            DE_loss = criterion(net(inputs), labels)\n",
    "\n",
    "            DE_loss.backward()\n",
    "\n",
    "            de_optimizers[i].step()\n",
    "\n",
    "    #         if epoch == 0 and j == 0:\n",
    "    #             print('initial loss: ', DE_loss.item())\n",
    "    # print('final loss: ', DE_loss.item())\n",
    "\n",
    "print('Finished training Deep ensembles:')\n",
    "\n",
    "# make predictions\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in zip(train_loader_x,train_loader_y):\n",
    "        inputs, labels = data\n",
    "        # reshape test data y\n",
    "        labels = labels.view(1)\n",
    "\n",
    "        outputs = DEmodel(inputs)\n",
    "\n",
    "        predicted = torch.round(outputs)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "print('Accuracy of the 100 train numbers of linear separable dataset (DE): %d %%' % (\n",
    "    100 * correct / total))\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in zip(test_loader_x,test_loader_y):\n",
    "        inputs, labels = data\n",
    "        # reshape test data y\n",
    "        labels = labels.view(1)\n",
    "\n",
    "        outputs = DEmodel(inputs)\n",
    "\n",
    "        predicted = torch.round(outputs)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "print('Accuracy of the 100 test numbers of linear separable dataset (DE): %d %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1\n",
      "Finished training bayesian NN:\n",
      "Accuracy of the 100 train numbers of linear separable dataset (Bayesian): 98 %\n",
      "Accuracy of the 100 test numbers of linear separable dataset (Bayesian): 100 %\n"
     ]
    }
   ],
   "source": [
    "######################################\n",
    "\n",
    "# 2.1 Performance test on Linear separable dataset\n",
    "\n",
    "######################################\n",
    "\n",
    "print('2.1')\n",
    "\n",
    "# use sklearn to generate separable data\n",
    "from sklearn.datasets import make_classification\n",
    "separable = False\n",
    "while not separable:\n",
    "    samples = make_classification(n_samples=200, n_features=5, n_informative=2, n_redundant=0)\n",
    "    red = samples[0][samples[1] == 0]\n",
    "    blue = samples[0][samples[1] == 1]\n",
    "    separable = any([red[:, k].max() < blue[:, k].min() or red[:, k].min() > blue[:, k].max() for k in range(5)])\n",
    "\n",
    "# split data\n",
    "train_x = samples[0][0:100,:]\n",
    "train_y = samples[1][0:100]\n",
    "test_x = samples[0][100:200,:]\n",
    "test_y = samples[1][100:200]\n",
    "\n",
    "# make dataset iterable\n",
    "train_loader_x = Variable(torch.Tensor(train_x))\n",
    "train_loader_y = Variable(torch.Tensor(train_y))\n",
    "test_loader_x = Variable(torch.Tensor(test_x))\n",
    "test_loader_y = Variable(torch.Tensor(test_y))\n",
    "\n",
    "# train the data\n",
    "\n",
    "iteration = 0\n",
    "for epoch in range(5):\n",
    "    for i, (datapoints, labels) in enumerate(zip(train_loader_x,train_loader_y)):\n",
    "\n",
    "        # reshape train data y\n",
    "        labels = labels.view(1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss = criterion(regressor(datapoints), labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "print('Finished training bayesian NN:')\n",
    "\n",
    "# make predictions\n",
    "test = []\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in zip(train_loader_x,train_loader_y):\n",
    "        inputs, labels = data\n",
    "        # reshape test data y\n",
    "        labels = labels.view(1)\n",
    "\n",
    "        outputs = regressor(inputs)\n",
    "        predicted = torch.round(outputs)\n",
    "        test.append(predicted)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "print('Accuracy of the 100 train numbers of linear separable dataset (Bayesian): %d %%' % (\n",
    "    100 * correct / total))\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in zip(test_loader_x,test_loader_y):\n",
    "        inputs, labels = data\n",
    "        # reshape test data y\n",
    "        labels = labels.view(1)\n",
    "\n",
    "        outputs = regressor(inputs)\n",
    "\n",
    "        predicted = torch.round(outputs)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "print('Accuracy of the 100 test numbers of linear separable dataset (Bayesian): %d %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2\n",
      "Accuracy of the 100 train numbers of linear non-separable dataset (LR): 76 %\n",
      "Accuracy of the 100 test numbers of linear non-separable dataset (LR): 84 %\n",
      "Finished training basic NN:\n",
      "Accuracy of the 100 train numbers of linear non-separable dataset (NN): 79 %\n",
      "Accuracy of the 100 test numbers of linear non-separable dataset (NN): 79 %\n",
      "Finished training Deep ensembles:\n",
      "Accuracy of the 100 train numbers of linear non-separable dataset (DE): 76 %\n",
      "Accuracy of the 100 test numbers of linear non-separable dataset (DE): 84 %\n"
     ]
    }
   ],
   "source": [
    "######################################\n",
    "\n",
    "# 1.2 Performance test on Linear non-separable dataset\n",
    "\n",
    "######################################\n",
    "\n",
    "print('1.2')\n",
    "\n",
    "# the linear non separable dataset is used the one for logistic regression\n",
    "\n",
    "# simulate a data set for a logistic regression model with 5 dimension:\n",
    "# assume covariance is an identity matrix\n",
    "sigma1 = np.identity(5)\n",
    "# mean is an array of 0\n",
    "mean1 = np.zeros(5)\n",
    "# number of samples are 200\n",
    "n1 = 200\n",
    "# generate n gaussian distributed data points\n",
    "x = np.random.multivariate_normal(mean1, sigma1, n1)\n",
    "\n",
    "# calculate p, set regression coefficients beta as an array of 1s\n",
    "p = 1/(1 + np.exp(-np.sum(x,axis = 1)))\n",
    "\n",
    "# split the training test data by half\n",
    "# simulate y by p = 0.5\n",
    "y = np.random.binomial(1,p,size= n1)\n",
    "\n",
    "# split data\n",
    "train_x = x[0:100,:]\n",
    "train_y = y[0:100]\n",
    "test_x = x[100:200,:]\n",
    "test_y = y[100:200]\n",
    "\n",
    "##############################\n",
    "# test on logistic regression\n",
    "##############################\n",
    "LR = LogisticRegression(solver='liblinear', random_state=0).fit(train_x, train_y)\n",
    "# accuracy score for train data\n",
    "print('Accuracy of the 100 train numbers of linear non-separable dataset (LR): %d %%' %\n",
    "      (100 * LR.score(train_x, train_y)))\n",
    "# predicted accuracy score for test data\n",
    "print('Accuracy of the 100 test numbers of linear non-separable dataset (LR): %d %%' %\n",
    "      (100 * LR.score(test_x, test_y)))\n",
    "\n",
    "##############################\n",
    "# test on basic neural network\n",
    "##############################\n",
    "\n",
    "# make dataset iterable\n",
    "train_loader_x = Variable(torch.Tensor(train_x))\n",
    "train_loader_y = Variable(torch.Tensor(train_y))\n",
    "test_loader_x = Variable(torch.Tensor(test_x))\n",
    "test_loader_y = Variable(torch.Tensor(test_y))\n",
    "\n",
    "# model training\n",
    "iter = 0\n",
    "for epoch in range(5):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(zip(train_loader_x,train_loader_y)):\n",
    "        inputs, labels = data\n",
    "        # reshape train data y\n",
    "        labels = labels.view(1)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        NN_optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        outputs = NNmodel(inputs)\n",
    "        # Compute Loss\n",
    "        NN_loss = criterion(outputs, labels)\n",
    "        # Backward pass\n",
    "        NN_loss.backward()\n",
    "        # optimize\n",
    "        NN_optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        # running_loss += loss.item()\n",
    "        # if i % 20 == 19:  # print every 20 mini-batches\n",
    "        #     print('[%d, %5d] loss: %.3f' %\n",
    "        #           (epoch + 1, i + 1, running_loss / 2000))\n",
    "        #     running_loss = 0.0\n",
    "\n",
    "print('Finished training basic NN:')\n",
    "\n",
    "# make predictions\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in zip(train_loader_x,train_loader_y):\n",
    "        inputs, labels = data\n",
    "        # reshape test data y\n",
    "        labels = labels.view(1)\n",
    "\n",
    "        outputs = NNmodel(inputs)\n",
    "\n",
    "        predicted = torch.round(outputs)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "print('Accuracy of the 100 train numbers of linear non-separable dataset (NN): %d %%' % (\n",
    "    100 * correct / total))\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in zip(test_loader_x,test_loader_y):\n",
    "        inputs, labels = data\n",
    "        # reshape test data y\n",
    "        labels = labels.view(1)\n",
    "\n",
    "        outputs = NNmodel(inputs)\n",
    "\n",
    "        predicted = torch.round(outputs)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "print('Accuracy of the 100 test numbers of linear non-separable dataset (NN): %d %%' % (\n",
    "    100 * correct / total))\n",
    "\n",
    "##############################\n",
    "# Training an ensemble of 5 MLPs with MSE\n",
    "##############################\n",
    "de = []\n",
    "de_optimizers = []\n",
    "for _ in range(5):\n",
    "    de.append(DEmodel)\n",
    "    de_optimizers.append(torch.optim.SGD(params=DEmodel.parameters(), lr=0.001))\n",
    "# train\n",
    "for i, net in enumerate(de):\n",
    "    # print('Training network ',i+1)\n",
    "    for epoch in range(5):\n",
    "        for j, data in enumerate(zip(train_loader_x, train_loader_y)):\n",
    "            inputs, labels = data\n",
    "            # reshape train data y\n",
    "            labels = labels.view(1)\n",
    "\n",
    "            de_optimizers[i].zero_grad()\n",
    "            # reshape train data y\n",
    "            DE_loss = criterion(net(inputs), labels)\n",
    "\n",
    "            DE_loss.backward()\n",
    "\n",
    "            de_optimizers[i].step()\n",
    "\n",
    "    #         if epoch == 0 and j == 0:\n",
    "    #             print('initial loss: ', DE_loss.item())\n",
    "    # print('final loss: ', DE_loss.item())\n",
    "\n",
    "print('Finished training Deep ensembles:')\n",
    "\n",
    "# make predictions\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in zip(train_loader_x,train_loader_y):\n",
    "        inputs, labels = data\n",
    "        # reshape test data y\n",
    "        labels = labels.view(1)\n",
    "\n",
    "        outputs = DEmodel(inputs)\n",
    "\n",
    "        predicted = torch.round(outputs)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "print('Accuracy of the 100 train numbers of linear non-separable dataset (DE): %d %%' % (\n",
    "    100 * correct / total))\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in zip(test_loader_x,test_loader_y):\n",
    "        inputs, labels = data\n",
    "        # reshape test data y\n",
    "        labels = labels.view(1)\n",
    "\n",
    "        outputs = DEmodel(inputs)\n",
    "\n",
    "        predicted = torch.round(outputs)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "print('Accuracy of the 100 test numbers of linear non-separable dataset (DE): %d %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2\n",
      "Finished training bayesian NN:\n",
      "Accuracy of the 100 train numbers of linear non-separable dataset (Bayesian): 80 %\n",
      "Accuracy of the 100 test numbers of linear non-separable dataset (Bayesian): 84 %\n"
     ]
    }
   ],
   "source": [
    "######################################\n",
    "\n",
    "# 2.2 Performance test on Linear non-separable dataset\n",
    "\n",
    "######################################\n",
    "\n",
    "print('2.2')\n",
    "\n",
    "# train the data\n",
    "\n",
    "iteration = 0\n",
    "for epoch in range(5):\n",
    "    for i, (datapoints, labels) in enumerate(zip(train_loader_x,train_loader_y)):\n",
    "\n",
    "        # reshape train data y\n",
    "        labels = labels.view(1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss = criterion(regressor(datapoints), labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "print('Finished training bayesian NN:')\n",
    "\n",
    "# make predictions\n",
    "test = []\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in zip(train_loader_x,train_loader_y):\n",
    "        inputs, labels = data\n",
    "        # reshape test data y\n",
    "        labels = labels.view(1)\n",
    "\n",
    "        outputs = regressor(inputs)\n",
    "        predicted = torch.round(outputs)\n",
    "        test.append(predicted)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "print('Accuracy of the 100 train numbers of linear non-separable dataset (Bayesian): %d %%' % (\n",
    "    100 * correct / total))\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in zip(test_loader_x,test_loader_y):\n",
    "        inputs, labels = data\n",
    "        # reshape test data y\n",
    "        labels = labels.view(1)\n",
    "\n",
    "        outputs = regressor(inputs)\n",
    "\n",
    "        predicted = torch.round(outputs)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "print('Accuracy of the 100 test numbers of linear non-separable dataset (Bayesian): %d %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3\n",
      "Accuracy of the 100 train numbers of non-linear separable dataset (LR): 99 %\n",
      "Accuracy of the 100 test numbers of non-linear separable dataset (LR): 100 %\n",
      "Finished training basic NN:\n",
      "Accuracy of the 100 train numbers of non-linear separable dataset (NN): 91 %\n",
      "Accuracy of the 100 test numbers of non-linear separable dataset (NN): 91 %\n",
      "Finished training Deep ensembles:\n",
      "Accuracy of the 100 train numbers of non-linear separable dataset (DE): 93 %\n",
      "Accuracy of the 100 test numbers of non-linear separable dataset (DE): 89 %\n"
     ]
    }
   ],
   "source": [
    "\n",
    "######################################\n",
    "\n",
    "# 1.3 Performance test on Non-linear separable dataset\n",
    "\n",
    "######################################\n",
    "\n",
    "print('1.3')\n",
    "\n",
    "# use sklearn to generate separable data\n",
    "separable = False\n",
    "while not separable:\n",
    "    samples = make_classification(n_samples=200, n_features=5, n_informative=2, n_redundant=0)\n",
    "    red = samples[0][samples[1] == 0]\n",
    "    blue = samples[0][samples[1] == 1]\n",
    "    sign = red.copy()\n",
    "    sign[sign < 0] = -1\n",
    "    sign[sign >= 0] = 1\n",
    "    red2 = np.square(red) * sign\n",
    "    separable = any([red2[:, k].max() < blue[:, k].min() or red2[:, k].min() > blue[:, k].max() for k in range(5)])\n",
    "\n",
    "# split data and calculate a non-linear dataset\n",
    "train_x = samples[0][0:100,:]\n",
    "train_y = samples[1][0:100]\n",
    "test_x = samples[0][100:200,:]\n",
    "test_y = samples[1][100:200]\n",
    "\n",
    "sign1 = train_x.copy()\n",
    "sign1[sign1 < 0] = -1\n",
    "sign1[sign1 >= 0] = 1\n",
    "train_x2 = np.square(train_x)*sign1\n",
    "\n",
    "sign2 = test_x.copy()\n",
    "sign2[sign2 < 0] = -1\n",
    "sign2[sign2 >= 0] = 1\n",
    "test_x2 = np.square(test_x)*sign2\n",
    "\n",
    "##############################\n",
    "# test on logistic regression\n",
    "##############################\n",
    "LR = LogisticRegression(solver='liblinear', random_state=0).fit(train_x2, train_y)\n",
    "# accuracy score for train data\n",
    "print('Accuracy of the 100 train numbers of non-linear separable dataset (LR): %d %%' %\n",
    "      (100 * LR.score(train_x2, train_y)))\n",
    "# predicted accuracy score for test data\n",
    "print('Accuracy of the 100 test numbers of non-linear separable dataset (LR): %d %%' %\n",
    "      (100 * LR.score(test_x2, test_y)))\n",
    "\n",
    "##############################\n",
    "# test on basic neural network\n",
    "##############################\n",
    "\n",
    "# make dataset iterable\n",
    "train_loader_x = Variable(torch.Tensor(train_x2))\n",
    "train_loader_y = Variable(torch.Tensor(train_y))\n",
    "test_loader_x = Variable(torch.Tensor(test_x2))\n",
    "test_loader_y = Variable(torch.Tensor(test_y))\n",
    "\n",
    "# model training\n",
    "iter = 0\n",
    "for epoch in range(5):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(zip(train_loader_x,train_loader_y)):\n",
    "        inputs, labels = data\n",
    "        # reshape train data y\n",
    "        labels = labels.view(1)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        NN_optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        outputs = NNmodel(inputs)\n",
    "        # Compute Loss\n",
    "        NN_loss = criterion(outputs, labels)\n",
    "        # Backward pass\n",
    "        NN_loss.backward()\n",
    "        # optimize\n",
    "        NN_optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        # running_loss += loss.item()\n",
    "        # if i % 20 == 19:  # print every 20 mini-batches\n",
    "        #     print('[%d, %5d] loss: %.3f' %\n",
    "        #           (epoch + 1, i + 1, running_loss / 2000))\n",
    "        #     running_loss = 0.0\n",
    "\n",
    "print('Finished training basic NN:')\n",
    "\n",
    "# make predictions\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in zip(train_loader_x,train_loader_y):\n",
    "        inputs, labels = data\n",
    "        # reshape test data y\n",
    "        labels = labels.view(1)\n",
    "\n",
    "        outputs = NNmodel(inputs)\n",
    "\n",
    "        predicted = torch.round(outputs)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "print('Accuracy of the 100 train numbers of non-linear separable dataset (NN): %d %%' % (\n",
    "    100 * correct / total))\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in zip(test_loader_x,test_loader_y):\n",
    "        inputs, labels = data\n",
    "        # reshape test data y\n",
    "        labels = labels.view(1)\n",
    "\n",
    "        outputs = NNmodel(inputs)\n",
    "\n",
    "        predicted = torch.round(outputs)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "print('Accuracy of the 100 test numbers of non-linear separable dataset (NN): %d %%' % (\n",
    "    100 * correct / total))\n",
    "\n",
    "##############################\n",
    "# Training an ensemble of 5 MLPs with MSE\n",
    "##############################\n",
    "de = []\n",
    "de_optimizers = []\n",
    "for _ in range(5):\n",
    "    de.append(DEmodel)\n",
    "    de_optimizers.append(torch.optim.SGD(params=DEmodel.parameters(), lr=0.001))\n",
    "# train\n",
    "for i, net in enumerate(de):\n",
    "    # print('Training network ',i+1)\n",
    "    for epoch in range(5):\n",
    "        for j, data in enumerate(zip(train_loader_x, train_loader_y)):\n",
    "            inputs, labels = data\n",
    "            # reshape train data y\n",
    "            labels = labels.view(1)\n",
    "\n",
    "            de_optimizers[i].zero_grad()\n",
    "            # reshape train data y\n",
    "            DE_loss = criterion(net(inputs), labels)\n",
    "\n",
    "            DE_loss.backward()\n",
    "\n",
    "            de_optimizers[i].step()\n",
    "\n",
    "    #         if epoch == 0 and j == 0:\n",
    "    #             print('initial loss: ', DE_loss.item())\n",
    "    # print('final loss: ', DE_loss.item())\n",
    "\n",
    "print('Finished training Deep ensembles:')\n",
    "\n",
    "# make predictions\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in zip(train_loader_x,train_loader_y):\n",
    "        inputs, labels = data\n",
    "        # reshape test data y\n",
    "        labels = labels.view(1)\n",
    "\n",
    "        outputs = DEmodel(inputs)\n",
    "\n",
    "        predicted = torch.round(outputs)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "print('Accuracy of the 100 train numbers of non-linear separable dataset (DE): %d %%' % (\n",
    "    100 * correct / total))\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in zip(test_loader_x,test_loader_y):\n",
    "        inputs, labels = data\n",
    "        # reshape test data y\n",
    "        labels = labels.view(1)\n",
    "\n",
    "        outputs = DEmodel(inputs)\n",
    "\n",
    "        predicted = torch.round(outputs)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "print('Accuracy of the 100 test numbers of non-linear separable dataset (DE): %d %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3\n",
      "Finished training bayesian NN:\n",
      "Accuracy of the 100 train numbers of non-linear separable dataset (Bayesian): 87 %\n",
      "Accuracy of the 100 test numbers of non-linear separable dataset (Bayesian): 91 %\n"
     ]
    }
   ],
   "source": [
    "######################################\n",
    "\n",
    "# 2.3 Performance test on Non-linear separable dataset\n",
    "\n",
    "######################################\n",
    "\n",
    "print('2.3')\n",
    "\n",
    "# train the data\n",
    "\n",
    "iteration = 0\n",
    "for epoch in range(5):\n",
    "    for i, (datapoints, labels) in enumerate(zip(train_loader_x,train_loader_y)):\n",
    "\n",
    "        # reshape train data y\n",
    "        labels = labels.view(1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss = criterion(regressor(datapoints), labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "print('Finished training bayesian NN:')\n",
    "\n",
    "# make predictions\n",
    "test = []\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in zip(train_loader_x,train_loader_y):\n",
    "        inputs, labels = data\n",
    "        # reshape test data y\n",
    "        labels = labels.view(1)\n",
    "\n",
    "        outputs = regressor(inputs)\n",
    "        predicted = torch.round(outputs)\n",
    "        test.append(predicted)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "print('Accuracy of the 100 train numbers of non-linear separable dataset (Bayesian): %d %%' % (\n",
    "    100 * correct / total))\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in zip(test_loader_x,test_loader_y):\n",
    "        inputs, labels = data\n",
    "        # reshape test data y\n",
    "        labels = labels.view(1)\n",
    "\n",
    "        outputs = regressor(inputs)\n",
    "\n",
    "        predicted = torch.round(outputs)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "print('Accuracy of the 100 test numbers of non-linear separable dataset (Bayesian): %d %%' % (\n",
    "    100 * correct / total))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4\n",
      "Accuracy of the 100 train numbers of non-linear non-separable dataset (LR): 87 %\n",
      "Accuracy of the 100 test numbers of non-linear non-separable dataset (LR): 82 %\n",
      "Finished training basic NN:\n",
      "Accuracy of the 100 train numbers of non-linear non-separable dataset (NN): 75 %\n",
      "Accuracy of the 100 test numbers of non-linear non-separable dataset (NN): 76 %\n",
      "Finished training Deep ensembles:\n",
      "Accuracy of the 100 train numbers of non-linear non-separable dataset (DE): 87 %\n",
      "Accuracy of the 100 test numbers of non-linear non-separable dataset (DE): 81 %\n"
     ]
    }
   ],
   "source": [
    "\n",
    "######################################\n",
    "\n",
    "# 1.4 Performance test on Non-linear non-separable dataset\n",
    "\n",
    "######################################\n",
    "\n",
    "print('1.4')\n",
    "\n",
    "# the non-linear non separable dataset is an expansion of the one for logistic regression\n",
    "\n",
    "# simulate a data set for a logistic regression model with 5 dimension:\n",
    "# assume covariance is an identity matrix\n",
    "sigma1 = np.identity(5)\n",
    "# mean is an array of 0\n",
    "mean1 = np.zeros(5)\n",
    "# number of samples are 200\n",
    "n1 = 200\n",
    "# generate n gaussian distributed data points\n",
    "x = np.random.multivariate_normal(mean1, sigma1, n1)\n",
    "\n",
    "# calculate p, set regression coefficients beta as an array of 1s and p to be non-linear by beta*x^2\n",
    "sign = x.copy()\n",
    "sign[sign < 0] = -1\n",
    "sign[sign >= 0] = 1\n",
    "x2 = np.square(x)*sign\n",
    "\n",
    "p = 1/(1 + np.exp(-np.sum(x2,axis = 1)))\n",
    "\n",
    "# split the training test data by half\n",
    "# simulate y by p = 0.5\n",
    "y = np.random.binomial(1,p,size= n1)\n",
    "\n",
    "# split data\n",
    "train_x = x[0:100,:]\n",
    "train_y = y[0:100]\n",
    "test_x = x[100:200,:]\n",
    "test_y = y[100:200]\n",
    "\n",
    "##############################\n",
    "# test on logistic regression\n",
    "##############################\n",
    "LR = LogisticRegression(solver='liblinear', random_state=0).fit(train_x, train_y)\n",
    "# accuracy score for train data\n",
    "print('Accuracy of the 100 train numbers of non-linear non-separable dataset (LR): %d %%' %\n",
    "      (100 * LR.score(train_x, train_y)))\n",
    "# predicted accuracy score for test data\n",
    "print('Accuracy of the 100 test numbers of non-linear non-separable dataset (LR): %d %%' %\n",
    "      (100 * LR.score(test_x, test_y)))\n",
    "\n",
    "##############################\n",
    "# test on basic neural network\n",
    "##############################\n",
    "\n",
    "# make dataset iterable\n",
    "train_loader_x = Variable(torch.Tensor(train_x))\n",
    "train_loader_y = Variable(torch.Tensor(train_y))\n",
    "test_loader_x = Variable(torch.Tensor(test_x))\n",
    "test_loader_y = Variable(torch.Tensor(test_y))\n",
    "\n",
    "# model training\n",
    "iter = 0\n",
    "for epoch in range(5):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(zip(train_loader_x,train_loader_y)):\n",
    "        inputs, labels = data\n",
    "        # reshape train data y\n",
    "        labels = labels.view(1)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        NN_optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        outputs = NNmodel(inputs)\n",
    "        # Compute Loss\n",
    "        NN_loss = criterion(outputs, labels)\n",
    "        # Backward pass\n",
    "        NN_loss.backward()\n",
    "        # optimize\n",
    "        NN_optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        # running_loss += loss.item()\n",
    "        # if i % 20 == 19:  # print every 20 mini-batches\n",
    "        #     print('[%d, %5d] loss: %.3f' %\n",
    "        #           (epoch + 1, i + 1, running_loss / 2000))\n",
    "        #     running_loss = 0.0\n",
    "\n",
    "print('Finished training basic NN:')\n",
    "\n",
    "# make predictions\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in zip(train_loader_x,train_loader_y):\n",
    "        inputs, labels = data\n",
    "        # reshape test data y\n",
    "        labels = labels.view(1)\n",
    "\n",
    "        outputs = NNmodel(inputs)\n",
    "\n",
    "        predicted = torch.round(outputs)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "print('Accuracy of the 100 train numbers of non-linear non-separable dataset (NN): %d %%' % (\n",
    "    100 * correct / total))\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in zip(test_loader_x,test_loader_y):\n",
    "        inputs, labels = data\n",
    "        # reshape test data y\n",
    "        labels = labels.view(1)\n",
    "\n",
    "        outputs = NNmodel(inputs)\n",
    "\n",
    "        predicted = torch.round(outputs)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "print('Accuracy of the 100 test numbers of non-linear non-separable dataset (NN): %d %%' % (\n",
    "    100 * correct / total))\n",
    "\n",
    "##############################\n",
    "# Training an ensemble of 5 MLPs with MSE\n",
    "##############################\n",
    "de = []\n",
    "de_optimizers = []\n",
    "for _ in range(5):\n",
    "    de.append(DEmodel)\n",
    "    de_optimizers.append(torch.optim.SGD(params=DEmodel.parameters(), lr=0.001))\n",
    "# train\n",
    "for i, net in enumerate(de):\n",
    "    # print('Training network ',i+1)\n",
    "    for epoch in range(5):\n",
    "        for j, data in enumerate(zip(train_loader_x, train_loader_y)):\n",
    "            inputs, labels = data\n",
    "            # reshape train data y\n",
    "            labels = labels.view(1)\n",
    "\n",
    "            de_optimizers[i].zero_grad()\n",
    "            # reshape train data y\n",
    "            DE_loss = criterion(net(inputs), labels)\n",
    "\n",
    "            DE_loss.backward()\n",
    "\n",
    "            de_optimizers[i].step()\n",
    "\n",
    "    #         if epoch == 0 and j == 0:\n",
    "    #             print('initial loss: ', DE_loss.item())\n",
    "    # print('final loss: ', DE_loss.item())\n",
    "\n",
    "print('Finished training Deep ensembles:')\n",
    "\n",
    "# make predictions\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in zip(train_loader_x,train_loader_y):\n",
    "        inputs, labels = data\n",
    "        # reshape test data y\n",
    "        labels = labels.view(1)\n",
    "\n",
    "        outputs = DEmodel(inputs)\n",
    "\n",
    "        predicted = torch.round(outputs)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "print('Accuracy of the 100 train numbers of non-linear non-separable dataset (DE): %d %%' % (\n",
    "    100 * correct / total))\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in zip(test_loader_x,test_loader_y):\n",
    "        inputs, labels = data\n",
    "        # reshape test data y\n",
    "        labels = labels.view(1)\n",
    "\n",
    "        outputs = DEmodel(inputs)\n",
    "\n",
    "        predicted = torch.round(outputs)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "print('Accuracy of the 100 test numbers of non-linear non-separable dataset (DE): %d %%' % (\n",
    "    100 * correct / total))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4\n",
      "Finished training bayesian NN:\n",
      "Accuracy of the 100 train numbers of non-linear non-separable dataset (Bayesian): 84 %\n",
      "Accuracy of the 100 test numbers of non-linear non-separable dataset (Bayesian): 81 %\n"
     ]
    }
   ],
   "source": [
    "######################################\n",
    "\n",
    "# 2.4 Performance test on Non-linear non-separable dataset\n",
    "\n",
    "######################################\n",
    "\n",
    "print('2.4')\n",
    "\n",
    "\n",
    "# train the data\n",
    "\n",
    "iteration = 0\n",
    "for epoch in range(5):\n",
    "    for i, (datapoints, labels) in enumerate(zip(train_loader_x,train_loader_y)):\n",
    "\n",
    "        # reshape train data y\n",
    "        labels = labels.view(1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss = criterion(regressor(datapoints), labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "print('Finished training bayesian NN:')\n",
    "\n",
    "# make predictions\n",
    "test = []\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in zip(train_loader_x,train_loader_y):\n",
    "        inputs, labels = data\n",
    "        # reshape test data y\n",
    "        labels = labels.view(1)\n",
    "\n",
    "        outputs = regressor(inputs)\n",
    "        predicted = torch.round(outputs)\n",
    "        test.append(predicted)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "print('Accuracy of the 100 train numbers of non-linear non-separable dataset (Bayesian): %d %%' % (\n",
    "    100 * correct / total))\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in zip(test_loader_x,test_loader_y):\n",
    "        inputs, labels = data\n",
    "        # reshape test data y\n",
    "        labels = labels.view(1)\n",
    "\n",
    "        outputs = regressor(inputs)\n",
    "\n",
    "        predicted = torch.round(outputs)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "print('Accuracy of the 100 test numbers of non-linear non-separable dataset (Bayesian): %d %%' % (\n",
    "    100 * correct / total))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
