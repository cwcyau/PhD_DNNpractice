{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################\n",
    "\n",
    "# Logistic regression and baic NN\n",
    "\n",
    "######################################\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Logistic regression\n",
    "# use multiple layers NN\n",
    "class LogisticDNN(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(LogisticDNN, self).__init__()\n",
    "        self.linear1 = nn.Linear(5, 60)\n",
    "        self.linear2 = nn.Linear(60, 20)\n",
    "        self.linear3 = nn.Linear(20, 10)\n",
    "        self.linear4 = nn.Linear(10, 5)\n",
    "        self.linear5 = nn.Linear(5, 1)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.linear3(x)\n",
    "        x = self.linear4(x)\n",
    "        x = self.linear5(x)\n",
    "        return x\n",
    "\n",
    "model = LogisticDNN()\n",
    "\n",
    "# define a loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1\n",
      "Accuracy of the 100 train numbers of linear separable dataset (LR): 100 %\n",
      "Accuracy of the 100 test numbers of linear separable dataset (LR): 99 %\n",
      "Finished Training\n",
      "Accuracy of the 100 train numbers of linear separable dataset (NN): 50 %\n",
      "Accuracy of the 100 test numbers of linear separable dataset (NN): 59 %\n"
     ]
    }
   ],
   "source": [
    "######################################\n",
    "\n",
    "# 1.1 Performance test on Linear separable dataset\n",
    "\n",
    "######################################\n",
    "\n",
    "print('1.1')\n",
    "\n",
    "# use sklearn to generate separable data\n",
    "from sklearn.datasets import make_classification\n",
    "separable = False\n",
    "while not separable:\n",
    "    samples = make_classification(n_samples=200, n_features=5, n_informative=2, n_redundant=0)\n",
    "    red = samples[0][samples[1] == 0]\n",
    "    blue = samples[0][samples[1] == 1]\n",
    "    separable = any([red[:, k].max() < blue[:, k].min() or red[:, k].min() > blue[:, k].max() for k in range(5)])\n",
    "\n",
    "# split data\n",
    "train_x = samples[0][0:100,:]\n",
    "train_y = samples[1][0:100]\n",
    "test_x = samples[0][100:200,:]\n",
    "test_y = samples[1][100:200]\n",
    "\n",
    "\n",
    "##############################\n",
    "# test on logistic regression\n",
    "##############################\n",
    "LR = LogisticRegression(solver='liblinear', random_state=0).fit(train_x, train_y)\n",
    "# accuracy score for train data\n",
    "print('Accuracy of the 100 train numbers of linear separable dataset (LR): %d %%' %\n",
    "      (100 * LR.score(train_x, train_y)))\n",
    "# predicted accuracy score for test data\n",
    "print('Accuracy of the 100 test numbers of linear separable dataset (LR): %d %%' %\n",
    "      (100 * LR.score(test_x, test_y)))\n",
    "\n",
    "##############################\n",
    "# test on basic neural network\n",
    "##############################\n",
    "\n",
    "# make dataset iterable\n",
    "train_loader_x = Variable(torch.Tensor(train_x))\n",
    "train_loader_y = Variable(torch.Tensor(train_y))\n",
    "test_loader_x = Variable(torch.Tensor(test_x))\n",
    "test_loader_y = Variable(torch.Tensor(test_y))\n",
    "\n",
    "# model training\n",
    "iter = 0\n",
    "for epoch in range(5):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(zip(train_loader_x,train_loader_y)):\n",
    "        inputs, labels = data\n",
    "        # reshape train data y\n",
    "        labels = labels.view(1)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        # Compute Loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        # optimize\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        # running_loss += loss.item()\n",
    "        # if i % 20 == 19:  # print every 20 mini-batches\n",
    "        #     print('[%d, %5d] loss: %.3f' %\n",
    "        #           (epoch + 1, i + 1, running_loss / 2000))\n",
    "        #     running_loss = 0.0\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "# make predictions\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in zip(train_loader_x,train_loader_y):\n",
    "        inputs, labels = data\n",
    "        # reshape test data y\n",
    "        labels = labels.view(1)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        predicted = torch.round(outputs)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "print('Accuracy of the 100 train numbers of linear separable dataset (NN): %d %%' % (\n",
    "    100 * correct / total))\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in zip(test_loader_x,test_loader_y):\n",
    "        inputs, labels = data\n",
    "        # reshape test data y\n",
    "        labels = labels.view(1)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        predicted = torch.round(outputs)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "print('Accuracy of the 100 test numbers of linear separable dataset (NN): %d %%' % (\n",
    "    100 * correct / total))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2\n",
      "Accuracy of the 100 train numbers of linear non-separable dataset (LR): 82 %\n",
      "Accuracy of the 100 test numbers of linear non-separable dataset (LR): 81 %\n",
      "Finished Training\n",
      "Accuracy of the 100 train numbers of linear non-separable dataset (NN): 62 %\n",
      "Accuracy of the 100 test numbers of linear non-separable dataset (NN): 63 %\n"
     ]
    }
   ],
   "source": [
    "\n",
    "######################################\n",
    "\n",
    "# 1.2 Performance test on Linear non-separable dataset\n",
    "\n",
    "######################################\n",
    "\n",
    "print('1.2')\n",
    "\n",
    "# the linear non separable dataset is used the one for logistic regression\n",
    "\n",
    "# simulate a data set for a logistic regression model with 5 dimension:\n",
    "# assume covariance is an identity matrix\n",
    "sigma1 = np.identity(5)\n",
    "# mean is an array of 0\n",
    "mean1 = np.zeros(5)\n",
    "# number of samples are 200\n",
    "n1 = 200\n",
    "# generate n gaussian distributed data points\n",
    "x = np.random.multivariate_normal(mean1, sigma1, n1)\n",
    "\n",
    "# calculate p, set regression coefficients beta as an array of 1s\n",
    "p = 1/(1 + np.exp(-np.sum(x,axis = 1)))\n",
    "\n",
    "# split the training test data by half\n",
    "# simulate y by p = 0.5\n",
    "y = np.random.binomial(1,p,size= n1)\n",
    "\n",
    "# split data\n",
    "train_x = x[0:100,:]\n",
    "train_y = y[0:100]\n",
    "test_x = x[100:200,:]\n",
    "test_y = y[100:200]\n",
    "\n",
    "##############################\n",
    "# test on logistic regression\n",
    "##############################\n",
    "LR = LogisticRegression(solver='liblinear', random_state=0).fit(train_x, train_y)\n",
    "# accuracy score for train data\n",
    "print('Accuracy of the 100 train numbers of linear non-separable dataset (LR): %d %%' %\n",
    "      (100 * LR.score(train_x, train_y)))\n",
    "# predicted accuracy score for test data\n",
    "print('Accuracy of the 100 test numbers of linear non-separable dataset (LR): %d %%' %\n",
    "      (100 * LR.score(test_x, test_y)))\n",
    "\n",
    "##############################\n",
    "# test on basic neural network\n",
    "##############################\n",
    "\n",
    "# make dataset iterable\n",
    "train_loader_x = Variable(torch.Tensor(train_x))\n",
    "train_loader_y = Variable(torch.Tensor(train_y))\n",
    "test_loader_x = Variable(torch.Tensor(test_x))\n",
    "test_loader_y = Variable(torch.Tensor(test_y))\n",
    "\n",
    "# model training\n",
    "iter = 0\n",
    "for epoch in range(5):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(zip(train_loader_x,train_loader_y)):\n",
    "        inputs, labels = data\n",
    "        # reshape train data y\n",
    "        labels = labels.view(1)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        # Compute Loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        # optimize\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        # running_loss += loss.item()\n",
    "        # if i % 20 == 19:  # print every 20 mini-batches\n",
    "        #     print('[%d, %5d] loss: %.3f' %\n",
    "        #           (epoch + 1, i + 1, running_loss / 2000))\n",
    "        #     running_loss = 0.0\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "# make predictions\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in zip(train_loader_x,train_loader_y):\n",
    "        inputs, labels = data\n",
    "        # reshape test data y\n",
    "        labels = labels.view(1)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        predicted = torch.round(outputs)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "print('Accuracy of the 100 train numbers of linear non-separable dataset (NN): %d %%' % (\n",
    "    100 * correct / total))\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in zip(test_loader_x,test_loader_y):\n",
    "        inputs, labels = data\n",
    "        # reshape test data y\n",
    "        labels = labels.view(1)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        predicted = torch.round(outputs)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "print('Accuracy of the 100 test numbers of linear non-separable dataset (NN): %d %%' % (\n",
    "    100 * correct / total))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3\n",
      "Accuracy of the 100 train numbers of non-linear separable dataset (LR): 98 %\n",
      "Accuracy of the 100 test numbers of non-linear separable dataset (LR): 98 %\n",
      "Finished Training\n",
      "Accuracy of the 100 train numbers of non-linear separable dataset (NN): 88 %\n",
      "Accuracy of the 100 test numbers of non-linear separable dataset (NN): 85 %\n"
     ]
    }
   ],
   "source": [
    "######################################\n",
    "\n",
    "# 1.3 Performance test on Non-linear separable dataset\n",
    "\n",
    "######################################\n",
    "\n",
    "print('1.3')\n",
    "\n",
    "# use sklearn to generate separable data\n",
    "separable = False\n",
    "while not separable:\n",
    "    samples = make_classification(n_samples=200, n_features=5, n_informative=2, n_redundant=0)\n",
    "    red = samples[0][samples[1] == 0]\n",
    "    blue = samples[0][samples[1] == 1]\n",
    "    sign = red.copy()\n",
    "    sign[sign < 0] = -1\n",
    "    sign[sign >= 0] = 1\n",
    "    red2 = np.square(red) * sign\n",
    "    separable = any([red2[:, k].max() < blue[:, k].min() or red2[:, k].min() > blue[:, k].max() for k in range(5)])\n",
    "\n",
    "# split data and calculate a non-linear dataset\n",
    "train_x = samples[0][0:100,:]\n",
    "train_y = samples[1][0:100]\n",
    "test_x = samples[0][100:200,:]\n",
    "test_y = samples[1][100:200]\n",
    "\n",
    "sign1 = train_x.copy()\n",
    "sign1[sign1 < 0] = -1\n",
    "sign1[sign1 >= 0] = 1\n",
    "train_x2 = np.square(train_x)*sign1\n",
    "\n",
    "sign2 = test_x.copy()\n",
    "sign2[sign2 < 0] = -1\n",
    "sign2[sign2 >= 0] = 1\n",
    "test_x2 = np.square(test_x)*sign2\n",
    "\n",
    "##############################\n",
    "# test on logistic regression\n",
    "##############################\n",
    "LR = LogisticRegression(solver='liblinear', random_state=0).fit(train_x2, train_y)\n",
    "# accuracy score for train data\n",
    "print('Accuracy of the 100 train numbers of non-linear separable dataset (LR): %d %%' %\n",
    "      (100 * LR.score(train_x2, train_y)))\n",
    "# predicted accuracy score for test data\n",
    "print('Accuracy of the 100 test numbers of non-linear separable dataset (LR): %d %%' %\n",
    "      (100 * LR.score(test_x2, test_y)))\n",
    "\n",
    "##############################\n",
    "# test on basic neural network\n",
    "##############################\n",
    "\n",
    "# make dataset iterable\n",
    "train_loader_x = Variable(torch.Tensor(train_x2))\n",
    "train_loader_y = Variable(torch.Tensor(train_y))\n",
    "test_loader_x = Variable(torch.Tensor(test_x2))\n",
    "test_loader_y = Variable(torch.Tensor(test_y))\n",
    "\n",
    "# model training\n",
    "iter = 0\n",
    "for epoch in range(5):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(zip(train_loader_x,train_loader_y)):\n",
    "        inputs, labels = data\n",
    "        # reshape train data y\n",
    "        labels = labels.view(1)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        # Compute Loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        # optimize\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        # running_loss += loss.item()\n",
    "        # if i % 20 == 19:  # print every 20 mini-batches\n",
    "        #     print('[%d, %5d] loss: %.3f' %\n",
    "        #           (epoch + 1, i + 1, running_loss / 2000))\n",
    "        #     running_loss = 0.0\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "# make predictions\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in zip(train_loader_x,train_loader_y):\n",
    "        inputs, labels = data\n",
    "        # reshape test data y\n",
    "        labels = labels.view(1)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        predicted = torch.round(outputs)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "print('Accuracy of the 100 train numbers of non-linear separable dataset (NN): %d %%' % (\n",
    "    100 * correct / total))\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in zip(test_loader_x,test_loader_y):\n",
    "        inputs, labels = data\n",
    "        # reshape test data y\n",
    "        labels = labels.view(1)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        predicted = torch.round(outputs)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "print('Accuracy of the 100 test numbers of non-linear separable dataset (NN): %d %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4\n",
      "Accuracy of the 100 train numbers of non-linear non-separable dataset (LR): 84 %\n",
      "Accuracy of the 100 test numbers of non-linear non-separable dataset (LR): 82 %\n",
      "Finished Training\n",
      "Accuracy of the 100 train numbers of non-linear non-separable dataset (NN): 79 %\n",
      "Accuracy of the 100 test numbers of non-linear non-separable dataset (NN): 77 %\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "######################################\n",
    "\n",
    "# 1.4 Performance test on Non-linear non-separable dataset\n",
    "\n",
    "######################################\n",
    "\n",
    "print('1.4')\n",
    "\n",
    "# the non-linear non separable dataset is an expansion of the one for logistic regression\n",
    "\n",
    "# simulate a data set for a logistic regression model with 5 dimension:\n",
    "# assume covariance is an identity matrix\n",
    "sigma1 = np.identity(5)\n",
    "# mean is an array of 0\n",
    "mean1 = np.zeros(5)\n",
    "# number of samples are 200\n",
    "n1 = 200\n",
    "# generate n gaussian distributed data points\n",
    "x = np.random.multivariate_normal(mean1, sigma1, n1)\n",
    "\n",
    "# calculate p, set regression coefficients beta as an array of 1s and p to be non-linear by beta*x^2\n",
    "sign = x.copy()\n",
    "sign[sign < 0] = -1\n",
    "sign[sign >= 0] = 1\n",
    "x2 = np.square(x)*sign\n",
    "\n",
    "p = 1/(1 + np.exp(-np.sum(x2,axis = 1)))\n",
    "\n",
    "# split the training test data by half\n",
    "# simulate y by p = 0.5\n",
    "y = np.random.binomial(1,p,size= n1)\n",
    "\n",
    "# split data\n",
    "train_x = x[0:100,:]\n",
    "train_y = y[0:100]\n",
    "test_x = x[100:200,:]\n",
    "test_y = y[100:200]\n",
    "\n",
    "##############################\n",
    "# test on logistic regression\n",
    "##############################\n",
    "LR = LogisticRegression(solver='liblinear', random_state=0).fit(train_x, train_y)\n",
    "# accuracy score for train data\n",
    "print('Accuracy of the 100 train numbers of non-linear non-separable dataset (LR): %d %%' %\n",
    "      (100 * LR.score(train_x, train_y)))\n",
    "# predicted accuracy score for test data\n",
    "print('Accuracy of the 100 test numbers of non-linear non-separable dataset (LR): %d %%' %\n",
    "      (100 * LR.score(test_x, test_y)))\n",
    "\n",
    "##############################\n",
    "# test on basic neural network\n",
    "##############################\n",
    "\n",
    "# make dataset iterable\n",
    "train_loader_x = Variable(torch.Tensor(train_x))\n",
    "train_loader_y = Variable(torch.Tensor(train_y))\n",
    "test_loader_x = Variable(torch.Tensor(test_x))\n",
    "test_loader_y = Variable(torch.Tensor(test_y))\n",
    "\n",
    "# model training\n",
    "iter = 0\n",
    "for epoch in range(5):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(zip(train_loader_x,train_loader_y)):\n",
    "        inputs, labels = data\n",
    "        # reshape train data y\n",
    "        labels = labels.view(1)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        # Compute Loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        # optimize\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        # running_loss += loss.item()\n",
    "        # if i % 20 == 19:  # print every 20 mini-batches\n",
    "        #     print('[%d, %5d] loss: %.3f' %\n",
    "        #           (epoch + 1, i + 1, running_loss / 2000))\n",
    "        #     running_loss = 0.0\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "# make predictions\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in zip(train_loader_x,train_loader_y):\n",
    "        inputs, labels = data\n",
    "        # reshape test data y\n",
    "        labels = labels.view(1)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        predicted = torch.round(outputs)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "print('Accuracy of the 100 train numbers of non-linear non-separable dataset (NN): %d %%' % (\n",
    "    100 * correct / total))\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in zip(test_loader_x,test_loader_y):\n",
    "        inputs, labels = data\n",
    "        # reshape test data y\n",
    "        labels = labels.view(1)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        predicted = torch.round(outputs)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "print('Accuracy of the 100 test numbers of non-linear non-separable dataset (NN): %d %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
